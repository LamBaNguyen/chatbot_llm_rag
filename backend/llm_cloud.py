import time
import threading
import queue
import sys
import select
from elasticsearch import Elasticsearch
# from model.embedding_model import get_embedding
# from model.embedding_model_new import get_embedding
from model.embedding_model_of_jina import get_embedding
from openai import OpenAI
import os
from dotenv import load_dotenv
import hashlib
import tiktoken

# ƒêo th·ªùi gian kh·ªüi ƒë·ªông
start_time = time.time()
print(f"Starting backend at {time.time() - start_time:.2f}s")

load_dotenv()
print(f"Loaded env at {time.time() - start_time:.2f}s")

# Kh·ªüi t·∫°o Elasticsearch
es = Elasticsearch(
    os.getenv("ELASTICSEARCH_URL"),
    api_key=os.getenv("ELASTICSEARCH_API_KEY")
)
print(f"Elasticsearch connected at {time.time() - start_time:.2f}s!")

INDEX_NAME = "chatbot_elastic"

token = os.getenv("GITHUB_TOKEN")
endpoint = "https://models.github.ai/inference"
model_name = "openai/gpt-4.1"
client = OpenAI(
    base_url=endpoint,
    api_key=token,
)
print(f"OpenAI client initialized at {time.time() - start_time:.2f}s!")

print(f"Backend fully initialized at {time.time() - start_time:.2f}s!")

# Kh·ªüi t·∫°o b·ªô m√£ h√≥a tiktoken
encoding = tiktoken.encoding_for_model("gpt-4o")

# function ƒë·ªÉ ƒë·ªïi model
def set_model_name(new_model_name: str):
    global model_name
    model_name = new_model_name
    print(f"‚úÖ Model name ƒë√£ ƒë·ªïi th√†nh: {model_name}")

def semantic_search(query, top_k=2, max_context_tokens=5000, stop_event=None):
    """
    T√¨m ki·∫øm ng·ªØ nghƒ©a trong Elasticsearch, tr·∫£ v·ªÅ danh s√°ch t√†i li·ªáu v√† context ƒë√£ ƒë∆∞·ª£c gi·ªõi h·∫°n token.
    Args:
        query (str): C√¢u truy v·∫•n.
        top_k (int): S·ªë l∆∞·ª£ng t√†i li·ªáu t·ªëi ƒëa tr·∫£ v·ªÅ.
        max_context_tokens (int): S·ªë token t·ªëi ƒëa cho ph√©p c·ªßa context.
        stop_event: S·ª± ki·ªán ƒë·ªÉ d·ª´ng t√°c v·ª• n·∫øu c·∫ßn.
    Returns:
        tuple: (danh s√°ch t√†i li·ªáu, l·ªói n·∫øu c√≥).
    """
    if stop_event and stop_event.is_set():
        return None, "T√°c v·ª• t√¨m ki·∫øm ƒë√£ b·ªã d·ª´ng."
    
    # Chuy·ªÉn query th√†nh ch·ªØ th∆∞·ªùng v√† t·∫°o embedding
    query = query.lower()
    vector = get_embedding(query)
    keywords = query.split()
    
    # X√¢y d·ª±ng truy v·∫•n Elasticsearch
    es_query = {
        "size": top_k,
        "query": {
            "bool": {
                "must": [
                    {
                         "match": {
                            "text": {
                                "query": query,
                                "operator": "or",
                                "minimum_should_match": "60%"
                            }
                        }
                    }
                ],
                "should": [
                    {
                        "match": {
                            "text": {
                                "query": query,
                                "operator": "or",
                                "fuzziness": "AUTO"
                            }
                        }
                    },
                    {
                        "script_score": {
                            "query": {"match_all": {}},
                            "script": {
                                "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                "params": {"query_vector": vector}
                            }
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        },
        "highlight": {
            "fields": {
                "text": {
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"]
                }
            }
        }
    }
    
    try:
        res = es.search(index=INDEX_NAME, body=es_query)
        if stop_event and stop_event.is_set():
            return None, "T√°c v·ª• t√¨m ki·∫øm ƒë√£ b·ªã d·ª´ng."
        # print(res["hits"]["hits"])
        if not res["hits"]["hits"]:
            return None, "M√¨nh kh√¥ng hi·ªÉu c√¢u n√†y l·∫Øm üò•. B·∫°n th·ª≠ h·ªèi ng·∫Øn g·ªçn h∆°n ^^"
        
        # T√¨m ƒëi·ªÉm s·ªë cao nh·∫•t
        # max_score = max(hit["_score"] for hit in res["hits"]["hits"])?
        
        # Ki·ªÉm tra n·∫øu ƒëi·ªÉm s·ªë cao nh·∫•t d∆∞·ªõi ng∆∞·ª°ng
        # if max_score < similarity_threshold:
        #     return None, "C√¢u h·ªèi n√†y kh√¥ng li√™n quan ƒë·∫øn du l·ªãch B√¨nh ƒê·ªãnh."
        
        # L·ªçc tr√πng l·∫∑p d·ª±a tr√™n n·ªôi dung t√†i li·ªáu
        results = res["hits"]["hits"]
        seen_hashes = set()
        unique_results = []
        for hit in results:
            source = hit['_source']
            doc_content = f"{source.get('title', '')}{source.get('content', '')}{source.get('link', '')}"
            doc_hash = hashlib.md5(doc_content.encode('utf-8')).hexdigest()
            if doc_hash not in seen_hashes:
                seen_hashes.add(doc_hash)
                unique_results.append(hit)
        
        if not unique_results:
            return None, "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ch·ª©a t·ª´ kh√≥a."
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (_score) ƒë·ªÉ ∆∞u ti√™n c√°c t√†i li·ªáu quan tr·ªçng
        sorted_results = sorted(unique_results, key=lambda x: x["_score"], reverse=True)
        
        # Ch·ªçn c√°c t√†i li·ªáu sao cho t·ªïng s·ªë token c·ªßa context kh√¥ng v∆∞·ª£t qu√° max_context_tokens
        selected_results = []
        current_tokens = 0
        
        for hit in sorted_results[:top_k]:
            # T·∫°o chu·ªói context cho t√†i li·ªáu n√†y
            context_snippet = f"**{hit['_source']['title']}**\n{hit['_source']['content']}\n{hit['_source']['link']}"
            snippet_tokens = len(encoding.encode(context_snippet))
            
            # N·∫øu th√™m t√†i li·ªáu n√†y v∆∞·ª£t qu√° gi·ªõi h·∫°n token, b·ªè qua
            if current_tokens + snippet_tokens > max_context_tokens:
                break
            
            selected_results.append(hit)
            current_tokens += snippet_tokens
        
        if not selected_results:
            return None, "Kh√¥ng t√¨m th·∫•y n·ªôi dung ph√π h·ª£p trong gi·ªõi h·∫°n token."
        
        # T·∫°o context t·ª´ c√°c t√†i li·ªáu ƒë√£ ch·ªçn
        context = "\n".join([f"**{hit['_source']['title']}**\n{hit['_source']['content']}\n{hit['_source']['link']}" for hit in selected_results])
        
        # In context ƒë·ªÉ ki·ªÉm tra (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn)
        print("===== K·∫æT QU·∫¢ ELASTICSEARCH T√åM ƒê∆Ø·ª¢C =====")
        print(context)
        print(f"S·ªë token c·ªßa context: {len(encoding.encode(context))}")
        # print("Max_score: ", max_score)
        return selected_results, None  # Tr·∫£ v·ªÅ danh s√°ch t√†i li·ªáu ƒë√£ ch·ªçn
    
    
    except Exception as e:
        return None, f"L·ªói Elasticsearch: {str(e)}"
    
def build_prompt(documents, query: str, max_context_tokens=5000) -> str:
    """
    X√¢y d·ª±ng prompt cho LLM, ƒë·∫£m b·∫£o t·ªïng s·ªë token c·ªßa context kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n.
    Args:
        documents: Danh s√°ch t√†i li·ªáu t·ª´ semantic_search.
        query (str): C√¢u truy v·∫•n.
        max_context_tokens (int): S·ªë token t·ªëi ƒëa cho context.
    Returns:
        str: Prompt ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u s·ªë token.
    """
    # Prompt h·ªá th·ªëng r√∫t g·ªçn
    system_prompt = (
        "B·∫°n l√† h∆∞·ªõng d·∫´n vi√™n du l·ªãch th√¢n thi·ªán, chuy√™n v·ªÅ B√¨nh ƒê·ªãnh, ƒë∆∞·ª£c t·∫°o b·ªüi **Nguy·ªÖn B√° L√¢m** (Ph√π M·ªπ, B√¨nh ƒê·ªãnh).\n"
        "Tr·∫£ l·ªùi g·∫ßn g≈©i, ƒë√∫ng tr·ªçng t√¢m, d√πng **Markdown**:\n"
        "- ∆Øu ti√™n d·ªØ li·ªáu b√™n d∆∞·ªõi, b·ªï sung ki·∫øn th·ª©c ngo√†i n·∫øu c·∫ßn (ghi r√µ).\n"
        "- N·∫øu ng∆∞·ªùi d√πng h·ªèi nhi·ªÅu √Ω (v√≠ d·ª•: ƒë·ªãa ƒëi·ªÉm + m√≥n ƒÉn), h√£y c·ªë g·∫Øng tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß c·∫£ hai n·∫øu li√™n quan ƒë·∫øn B√¨nh ƒê·ªãnh."
        "- Gi·ªØ c√¢u tr·∫£ l·ªùi ~400 token, d√πng g·∫°ch ƒë·∫ßu d√≤ng (-), in ƒë·∫≠m **ti√™u ƒë·ªÅ**.\n"
        "- T·ª´ ch·ªëi l·ªãch s·ª± n·∫øu kh√¥ng li√™n quan ƒë·∫øn du l·ªãch, vƒÉn h√≥a, l·ªãch s·ª≠ B√¨nh ƒê·ªãnh.\n"
        "- Ch·ªâ d√πng li√™n k·∫øt c·ªßa t√†i li·ªáu ƒë·∫ßu ti√™n.\n"
    )
    
    if documents:
        # T·∫°o context v·ªõi th√¥ng tin t√†i li·ªáu
        context_parts = []
        current_tokens = 0
        
        for doc in documents:
            snippet = f"**{doc['_source']['title']}**\n{doc['_source']['content']}"
            snippet_tokens = len(encoding.encode(snippet))
            
            if current_tokens + snippet_tokens > max_context_tokens:
                break
                
            context_parts.append(snippet)
            current_tokens += snippet_tokens
        
        context = "\n\n".join(context_parts)
        
        prompt = (
            f"{system_prompt}\n\n"
            f"**D·ªØ li·ªáu ch√≠nh:**\n{context}\n\n"
            f"**C√¢u h·ªèi:** {query}\n\n"
        )
        
        # Ki·ªÉm tra t·ªïng s·ªë token c·ªßa prompt (kh√¥ng t√≠nh link)
        total_tokens = len(encoding.encode(prompt))
        print(f"S·ªë token c·ªßa prompt (kh√¥ng t√≠nh history): {total_tokens}")
        
        return prompt
    else:
        return (
            f"{system_prompt}\n\n"
            f"**Xin l·ªói nh√©!** M√¨nh kh√¥ng c√≥ d·ªØ li·ªáu ch√≠nh v·ªÅ ‚Äú{query}‚Äù. "
            "H·ªèi m√¨nh v·ªÅ du l·ªãch B√¨nh ƒê·ªãnh nh√©! üòä"
        )
    
def truncate_text(text, max_tokens):
    """
    C·∫Øt b·ªõt vƒÉn b·∫£n ƒë·ªÉ kh√¥ng v∆∞·ª£t qu√° s·ªë token cho ph√©p.
    Args:
        text (str): VƒÉn b·∫£n c·∫ßn c·∫Øt.
        max_tokens (int): S·ªë token t·ªëi ƒëa.
    Returns:
        str: VƒÉn b·∫£n ƒë√£ c·∫Øt b·ªõt.
    """
    if not text:
        return text
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return text
    truncated_tokens = tokens[:max_tokens]
    return encoding.decode(truncated_tokens)

def generate_response(query, documents, history, client, stop_event=None, max_total_tokens=8000):
    """
    T·∫°o ph·∫£n h·ªìi t·ª´ GPT-4o-mini, ƒë·∫£m b·∫£o t·ªïng s·ªë token kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n.
    Args:
        query (str): C√¢u truy v·∫•n.
        documents: Danh s√°ch t√†i li·ªáu t·ª´ semantic_search.
        history: L·ªãch s·ª≠ h·ªôi tho·∫°i.
        client: Client ƒë·ªÉ g·ªçi LLM.
        stop_event: S·ª± ki·ªán ƒë·ªÉ d·ª´ng t√°c v·ª•.
        max_total_tokens (int): S·ªë token t·ªëi ƒëa cho to√†n b·ªô messages.
    Returns:
        tuple: (ph·∫£n h·ªìi, l·ªói n·∫øu c√≥).
    """
    if stop_event and stop_event.is_set():
        return None, "T√°c v·ª• tr·∫£ l·ªùi ƒë√£ b·ªã d·ª´ng."
    
    try:
        # X√¢y d·ª±ng prompt
        prompt = build_prompt(documents, query, max_context_tokens=5000)
        
        # T·∫°o messages
        messages = [{"role": "system", "content": prompt}]
        
        # Gi·ªõi h·∫°n history (ch·ªâ l·∫•y 5 tin nh·∫Øn g·∫ßn nh·∫•t)
        history = history[-5:] if len(history) > 5 else history
        
        # Th√™m history v√†o messages, c·∫Øt b·ªõt n·∫øu c·∫ßn
        history_tokens = 0
        truncated_history = []
        for msg in history:
            role = msg["role"]
            if role == "bot":
                role = "assistant"
            content = msg["content"]
            msg_tokens = len(encoding.encode(content))
            
            if history_tokens + msg_tokens > 1000:  # Gi·ªõi h·∫°n history d∆∞·ªõi 1000 token
                content = truncate_text(content, 1000 - history_tokens)
                msg_tokens = len(encoding.encode(content))
            
            history_tokens += msg_tokens
            truncated_history.append({"role": role, "content": content})
        
        messages.extend(truncated_history)
        messages.append({"role": "user", "content": query})
        
        # T√≠nh t·ªïng s·ªë token c·ªßa messages
        total_tokens = sum(len(encoding.encode(msg["content"])) for msg in messages)
        print(f"T·ªïng s·ªë token c·ªßa messages (ban ƒë·∫ßu): {total_tokens}")
        
        # N·∫øu v∆∞·ª£t qu√° gi·ªõi h·∫°n, c·∫Øt b·ªõt context v√† query
        if total_tokens > max_total_tokens:
            # ∆Ø·ªõc t√≠nh s·ªë token c·ªßa system prompt v√† history
            system_tokens = len(encoding.encode(messages[0]["content"]))
            history_tokens = sum(len(encoding.encode(msg["content"])) for msg in messages[1:-1])
            query_tokens = len(encoding.encode(query))
            
            # S·ªë token c√≤n l·∫°i cho context
            remaining_tokens = max_total_tokens - (system_tokens + history_tokens + query_tokens)
            if remaining_tokens < 1000:  # N·∫øu kh√¥ng ƒë·ªß ch·ªó cho context, c·∫Øt b·ªõt query
                remaining_tokens = max(1000, remaining_tokens)
                max_query_tokens = max_total_tokens - (system_tokens + history_tokens + remaining_tokens)
                query = truncate_text(query, max_query_tokens)
                messages[-1]["content"] = query
                query_tokens = len(encoding.encode(query))
                remaining_tokens = max_total_tokens - (system_tokens + history_tokens + query_tokens)
            
            # C·∫Øt b·ªõt context trong prompt
            if documents:
                context_parts = []
                current_tokens = 0
                for doc in documents:
                    snippet = f"**{doc['_source']['title']}**\n{doc['_source']['content']}"
                    snippet_tokens = len(encoding.encode(snippet))
                    if current_tokens + snippet_tokens > remaining_tokens:
                        break
                    context_parts.append(snippet)
                    current_tokens += snippet_tokens
                
                context = "\n\n".join(context_parts)
                messages[0]["content"] = (
                    f"{messages[0]['content'].split('**D·ªØ li·ªáu ch√≠nh:**')[0]}"
                    f"**D·ªØ li·ªáu ch√≠nh:**\n{context}\n\n"
                    f"**C√¢u h·ªèi:** {query}\n\n"
                )
            
            total_tokens = sum(len(encoding.encode(msg["content"])) for msg in messages)
            print(f"T·ªïng s·ªë token c·ªßa messages (sau khi c·∫Øt): {total_tokens}")
        
        # G·ª≠i ƒë·∫øn LLM
        result_queue = queue.Queue()
        
        def run_openai():
            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=500,
                    temperature=0.5,
                )
                result_queue.put((response.choices[0].message.content, None))
            except Exception as e:
                result_queue.put((None, f"L·ªói LLM: {str(e)}"))
        
        openai_thread = threading.Thread(target=run_openai)
        openai_thread.start()
        openai_thread.join(timeout=30)
        
        if openai_thread.is_alive():
            if stop_event:
                stop_event.set()
            return None, "M√¨nh x·ª≠ l√Ω h∆°i l√¢u, b·∫°n h·ªèi l·∫°i nh√©!"
        
        content, error = result_queue.get()
        if error:
            return None, error
        
        if stop_event and stop_event.is_set():
            return None, "T√°c v·ª• tr·∫£ l·ªùi ƒë√£ b·ªã d·ª´ng."
        
        # Th√™m li√™n k·∫øt c·ªßa t√†i li·ªáu ƒë·∫ßu ti√™n
        if documents:
            first_doc = documents[0]['_source']
            content += f'\n\n<a href="{first_doc["link"]}">ƒê·ªçc th√™m t·∫°i ƒë√¢y nh√©üòä</a>'
        
        # ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi k·∫øt th√∫c t·ª± nhi√™n
        if content.endswith("...") or not content.strip().endswith(("!", ".", "?", "üòä")):
            last_punct = max(content.rfind(p) for p in [".", "!", "?", "üòä"])
            if last_punct != -1:
                content = content[:last_punct + 1]
        
        return content, None
    
    except Exception as e:
        return None, f"L·ªói trong generate_response: {str(e)}"
       
def process_query(query, history, result_queue, stop_event):
     # Ph√¢n lo·∫°i c√¢u h·ªèi
    label = classify_query_intent(query, client)
    
    if label == "greeting":
        result_queue.put((
            "ü•∞ Ch√†o b·∫°n nha! M√¨nh lu√¥n s·∫µn s√†ng h·ªó tr·ª£ n·∫øu b·∫°n c·∫ßn t√¨m hi·ªÉu v·ªÅ du l·ªãch B√¨nh ƒê·ªãnh n√®!",
            None
        ))
        return
    
    if label == "unrelated":
        result_queue.put((
            "üò• Xin l·ªói, c√¢u h·ªèi c·ªßa b·∫°n n·∫±m ngo√†i lƒ©nh v·ª±c du l·ªãch, vƒÉn h√≥a, l·ªãch s·ª≠ B√¨nh ƒê·ªãnh.\n"
            "B·∫°n th·ª≠ h·ªèi m√¨nh nh·ªØng c√¢u li√™n quan ƒë·∫øn v√πng ƒë·∫•t n√†y nha!",
            None
        ))
        return
    #Yes
    # T√¨m ki·∫øm ng·ªØ nghƒ©a
    documents, error = semantic_search(query, stop_event=stop_event)
    if stop_event.is_set():
        result_queue.put((None, "M√¨nh x·ª≠ l√Ω h∆°i l√¢u, b·∫°n h·ªèi l·∫°i nh√©!"))
        return
        
    if error:
        result_queue.put((None, error))
        documents = None

    # T·∫°o c√¢u tr·∫£ l·ªùi
    response, error = generate_response(query, documents, history, client, stop_event=stop_event)
    if stop_event.is_set():
        result_queue.put((None, "M√¨nh x·ª≠ l√Ω h∆°i l√¢u, b·∫°n h·ªèi l·∫°i nh√©!"))
        return
        
    if error:
        result_queue.put((None, error))
    else:
        result_queue.put((response, None))

def check_for_stop(timeout=0.1):
    if sys.stdin in select.select([sys.stdin], [], [], timeout)[0]:
        line = sys.stdin.readline().strip()
        if line.lower() == "stop":
            return True
    return False

#h√†m d√πng GPT ƒë√°nh gi√° c√¢u h·ªèi c√≥ thu·ªôc lƒ©nh v·ª±c chatbot h·ªçc kh√¥ng?
def classify_query_intent(query: str, client) -> str:
    """
    Ph√¢n lo·∫°i c√¢u h·ªèi th√†nh:
    - 'related': li√™n quan ƒë·∫øn du l·ªãch, vƒÉn h√≥a, l·ªãch s·ª≠ ·ªü B√¨nh ƒê·ªãnh
    - 'unrelated': kh√¥ng li√™n quan
    - 'greeting': l·ªùi ch√†o, c·∫£m ∆°n, t·∫°m bi·ªát, x√£ giao
    """
    system_prompt = (
        "B·∫°n l√† b·ªô ph√¢n lo·∫°i th√¥ng minh. Ph√¢n lo·∫°i c√¢u h·ªèi ƒë·∫ßu v√†o th√†nh 1 trong 3 lo·∫°i sau:\n"
        "- related: n·∫øu li√™n quan ƒë·∫øn du l·ªãch, vƒÉn h√≥a, l·ªãch s·ª≠, ·∫©m th·ª±c, ƒÉn u·ªëng, vui ch∆°i ·ªü B√¨nh ƒê·ªãnh\n"
        "- unrelated: n·∫øu kh√¥ng li√™n quan g√¨ ƒë·∫øn ch·ªß ƒë·ªÅ tr√™n\n"
        "- greeting: n·∫øu l√† l·ªùi ch√†o h·ªèi, c·∫£m ∆°n, ch√∫c, t·∫°m bi·ªát, v.v.\n\n"
        "Ch·ªâ tr·∫£ l·ªùi 1 t·ª´: related / unrelated / greeting.\n"
        "V√≠ d·ª•:\n"
        "- 'C√≥ nh·ªØng ƒë·ªãa ƒëi·ªÉm du l·ªãch n√†o ·ªü Quy Nh∆°n?' ‚Üí related\n"
        "- 'T√¥i n√™n h·ªçc Python ·ªü ƒë√¢u?' ‚Üí unrelated\n"
        "- 'Ch√†o b·∫°n!' ‚Üí greeting\n"
        "- 'T·∫°m bi·ªát nh√©, h·∫πn g·∫∑p l·∫°i!' ‚Üí greeting\n"
        "- 'Ngh·ªá thu·∫≠t h√°t tu·ªìng ·ªü B√¨nh ƒê·ªãnh ra sao?' ‚Üí related\n"
        "- 'iPhone 15 ra m·∫Øt nƒÉm n√†o?' ‚Üí unrelated\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"C√¢u h·ªèi: {query}\nTr·∫£ l·ªùi (related/unrelated/greeting):"}
    ]

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=5,
            temperature=0,
        )
        label = response.choices[0].message.content.strip().lower()
        print(f"[DEBUG] GPT ph√¢n lo·∫°i '{query}' ‚Üí {label}")
        return label if label in ["related", "unrelated", "greeting"] else "related"
    except Exception as e:
        print("L·ªói khi ph√¢n lo·∫°i c√¢u h·ªèi:", str(e))
        return "related"

def chatbot():
    history = []
    print("üí¨ Ch√†o b·∫°n! M√¨nh l√† chatbot du l·ªãch B√¨nh ƒê·ªãnh. H·ªèi m√¨nh b·∫•t c·ª© ƒëi·ªÅu g√¨ nh√©! (Nh·∫≠p 'exit' ƒë·ªÉ tho√°t, ho·∫∑c 'stop' ƒë·ªÉ d·ª´ng t√°c v·ª• ƒëang ch·∫°y)")
    
    while True:
        query = input("üîç Nh·∫≠p c√¢u h·ªèi: ")
        if query.lower() == 'exit':
            print("üëã T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i b·∫°n!")
            break
        
        # T·∫°o stop event v√† queue
        stop_event = threading.Event()
        result_queue = queue.Queue()

        # Ch·∫°y x·ª≠ l√Ω query trong lu·ªìng ri√™ng
        process_thread = threading.Thread(target=process_query, args=(query, history, result_queue, stop_event))
        process_thread.start()

        # ƒê·ª£i thread ho√†n th√†nh
        process_thread.join()
        response, error = result_queue.get()

        if error:
            print(error)
        else:
            print("üí¨ Tr·∫£ l·ªùi:", response)
            history.append({"role": "user", "content": query})
            history.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    chatbot()